# Senior Team Lead Coding Test

**Duration: 60 minutes**
**Focus: Real-time Stream Processing, Async Programming, Error Handling**

## ğŸ¯ Your Challenge

You need to implement a **real-time data processing pipeline** that can handle streaming data from multiple sources
concurrently while dealing with rate limits and failures gracefully.

## ğŸ“‹ What You Need to Do

### 1. Implement the `StreamProcessor` class

In the file `data_streaming_pipeline_test.py`, you'll find a `StreamProcessor` class with empty methods. Your job is to
implement all the methods to make the pipeline work.

### 2. Key Requirements

Your implementation must:

- âœ… **Process multiple data streams concurrently** (not one at a time)
- âœ… **Handle rate limiting** - respect the downstream API's rate limits without crashing
- âœ… **Recover from failures** - API calls will fail ~15% of the time, handle this gracefully
- âœ… **Provide real-time metrics** - track processing statistics as data flows through
- âœ… **Process efficiently** - achieve minimum 5 requests/sec throughput
- âœ… **Manage backpressure** - handle when data comes in faster than you can process

### 3. What Success Looks Like

When you run `python data_streaming_pipeline_test.py`, you should see:

```
ğŸš€ Testing Stream Processing Pipeline...
ğŸ“Š Starting stream processing for 10 seconds...
âœ… Processing completed!
ğŸ“ˆ Total processed: 45
âš¡ Processing rate: 6.2/sec
ğŸ• Avg processing time: 150.3ms
âŒ Error rate: 12.5%
ğŸ“¦ Final backlog: 3
ğŸš¦ Rate limit hits: 2
ğŸ‰ All performance requirements met!
```

**Performance Targets:**

- **Processing rate**: â‰¥ 5/sec
- **Error rate**: â‰¤ 30%
- **Backlog size**: â‰¤ 100 items

## ğŸ—ï¸ Architecture Overview

```
Data Streams â†’ [Your StreamProcessor] â†’ Downstream API
     â†“                    â†“                    â†“
 Multiple sources    Process & validate    Rate limited
 Real-time data      Handle failures       Random failures
```

**Pipeline Flow:**
1. Consume data from the provided `data_stream`
2. Transform each `StreamData` using `transform_data()`
3. Send `ProcessedRecord` to `downstream_api.send_data()`
4. Track metrics throughout the process
5. Handle rate limits and failures with retries

## ğŸ”§ Files You'll Work With

### `data_streaming_pipeline_test.py` - **YOUR MAIN FILE**

- Contains the `StreamProcessor` class you need to implement
- Has a built-in test runner at the bottom
- **This is the only file you should modify**

### `streaming_test_framework.py` - **DO NOT MODIFY**

- Contains all the supporting infrastructure (models, simulators, etc.)
- Provides the data types you'll work with:
    - `StreamData` - incoming data from streams
    - `ProcessedRecord` - your processed output
    - `PipelineMetrics` - statistics you need to track
    - `DownstreamAPI` - the API you send data to (with rate limits and failures)

## ğŸ’¡ Key Concepts

Your solution will need to handle:

- **Streaming data processing** - continuous data flow from multiple sources
- **Error resilience** - APIs fail, your system shouldn't
- **Performance constraints** - meet throughput requirements under realistic conditions
- **Resource management** - handle concurrent processing efficiently

## ğŸš€ Getting Started

1. **Understand the interface**: Look at the `StreamProcessor` class methods
2. **Run the test first**: `python data_streaming_pipeline_test.py` (it will fail initially)
3. **Implement step by step**: Start with basic functionality, then add concurrency and error handling
4. **Test frequently**: Run the test after each major change to see progress
5. **Focus on the core challenge**: Async stream processing with concurrency and resilience

## âš ï¸ Important Notes

- **Only modify** `data_streaming_pipeline_test.py`
- **Don't modify** `streaming_test_framework.py`
- **You can use** standard Python libraries (asyncio, collections, etc.)
- **Ask questions** if anything is unclear about the requirements
- **Focus on working code first**, then optimize for performance

---

**Good luck! Focus on building a robust, concurrent stream processor. Remember: working code first, then optimization.**
ğŸš€
